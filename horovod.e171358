[00;34m 	'gcc/10.2.0' supports the following modules [0m
	{MPI}
	'mpi/openmpi-3.1.5' 'mpi/openmpi-4.1.1' 'mpi/mvapich2-2.3.6'
	{CUDA_MPI}
	'cudampi/openmpi-3.1.5' 'cudampi/openmpi-4.1.1' 'cudampi/mvapich2-2.3.6'
[00;31m 	{CUDA_MPI} is only supported in cuda 11.4 version. [0m
	{libraries}
	'hdf4/4.2.13' 'hdf5/1.10.2' 'hdf5/1.12.0' 'lapack/3.7.0' 'lapack/3.10.0' 'ncl/6.5.0' 'netcdf/4.6.1'
[00;34m 	'cuda/11.4' supports the {CUDA_MPI}. [0m
[1,3]<stderr>:/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[1,3]<stderr>:  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[1,5]<stderr>:/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[1,5]<stderr>:  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[1,6]<stderr>:/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[1,6]<stderr>:  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[1,0]<stderr>:/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[1,0]<stderr>:  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[1,1]<stderr>:/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[1,1]<stderr>:  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[1,7]<stderr>:/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[1,7]<stderr>:  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[1,2]<stderr>:/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[1,2]<stderr>:  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[1,4]<stderr>:/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
[1,4]<stderr>:  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[1,1]<stderr>:Traceback (most recent call last):
[1,1]<stderr>:  File "main.py", line 39, in <module>
[1,1]<stderr>:    train(args)
[1,1]<stderr>:  File "/scratch/hpc90a02/SRGANT/tk4h/mode.py", line 175, in train
[1,1]<stderr>:    g_optim.zero_grad()
[1,1]<stderr>:  File "/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/horovod/torch/optimizer.py", line 339, in zero_grad
[1,1]<stderr>:    raise AssertionError("optimizer.zero_grad() was called after loss.backward() "
[1,1]<stderr>:AssertionError: optimizer.zero_grad() was called after loss.backward() but before optimizer.step() or optimizer.synchronize(). This is prohibited as it can cause a race condition.
[1,0]<stderr>:Traceback (most recent call last):
[1,0]<stderr>:  File "main.py", line 39, in <module>
[1,0]<stderr>:    train(args)
[1,0]<stderr>:  File "/scratch/hpc90a02/SRGANT/tk4h/mode.py", line 175, in train
[1,0]<stderr>:    g_optim.zero_grad()
[1,0]<stderr>:  File "/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/horovod/torch/optimizer.py", line 339, in zero_grad
[1,0]<stderr>:    raise AssertionError("optimizer.zero_grad() was called after loss.backward() "
[1,0]<stderr>:AssertionError: optimizer.zero_grad() was called after loss.backward() but before optimizer.step() or optimizer.synchronize(). This is prohibited as it can cause a race condition.
[1,3]<stderr>:Traceback (most recent call last):
[1,3]<stderr>:  File "main.py", line 39, in <module>
[1,3]<stderr>:    train(args)
[1,3]<stderr>:  File "/scratch/hpc90a02/SRGANT/tk4h/mode.py", line 175, in train
[1,3]<stderr>:    g_optim.zero_grad()
[1,3]<stderr>:  File "/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/horovod/torch/optimizer.py", line 339, in zero_grad
[1,3]<stderr>:    raise AssertionError("optimizer.zero_grad() was called after loss.backward() "
[1,3]<stderr>:AssertionError: optimizer.zero_grad() was called after loss.backward() but before optimizer.step() or optimizer.synchronize(). This is prohibited as it can cause a race condition.
[1,4]<stderr>:Traceback (most recent call last):
[1,4]<stderr>:  File "main.py", line 39, in <module>
[1,4]<stderr>:    train(args)
[1,4]<stderr>:  File "/scratch/hpc90a02/SRGANT/tk4h/mode.py", line 175, in train
[1,4]<stderr>:    g_optim.zero_grad()
[1,4]<stderr>:  File "/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/horovod/torch/optimizer.py", line 339, in zero_grad
[1,4]<stderr>:    raise AssertionError("optimizer.zero_grad() was called after loss.backward() "
[1,4]<stderr>:AssertionError: optimizer.zero_grad() was called after loss.backward() but before optimizer.step() or optimizer.synchronize(). This is prohibited as it can cause a race condition.
[1,6]<stderr>:Traceback (most recent call last):
[1,6]<stderr>:  File "main.py", line 39, in <module>
[1,6]<stderr>:    train(args)
[1,6]<stderr>:  File "/scratch/hpc90a02/SRGANT/tk4h/mode.py", line 175, in train
[1,6]<stderr>:    g_optim.zero_grad()
[1,6]<stderr>:  File "/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/horovod/torch/optimizer.py", line 339, in zero_grad
[1,6]<stderr>:    raise AssertionError("optimizer.zero_grad() was called after loss.backward() "
[1,6]<stderr>:AssertionError: optimizer.zero_grad() was called after loss.backward() but before optimizer.step() or optimizer.synchronize(). This is prohibited as it can cause a race condition.
[1,5]<stderr>:Traceback (most recent call last):
[1,5]<stderr>:  File "main.py", line 39, in <module>
[1,5]<stderr>:    train(args)
[1,5]<stderr>:  File "/scratch/hpc90a02/SRGANT/tk4h/mode.py", line 175, in train
[1,5]<stderr>:    g_optim.zero_grad()
[1,5]<stderr>:  File "/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/horovod/torch/optimizer.py", line 339, in zero_grad
[1,5]<stderr>:    raise AssertionError("optimizer.zero_grad() was called after loss.backward() "
[1,5]<stderr>:AssertionError: optimizer.zero_grad() was called after loss.backward() but before optimizer.step() or optimizer.synchronize(). This is prohibited as it can cause a race condition.
[1,2]<stderr>:Traceback (most recent call last):
[1,2]<stderr>:  File "main.py", line 39, in <module>
[1,2]<stderr>:    train(args)
[1,2]<stderr>:  File "/scratch/hpc90a02/SRGANT/tk4h/mode.py", line 175, in train
[1,2]<stderr>:    g_optim.zero_grad()
[1,2]<stderr>:  File "/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/horovod/torch/optimizer.py", line 339, in zero_grad
[1,2]<stderr>:    raise AssertionError("optimizer.zero_grad() was called after loss.backward() "
[1,2]<stderr>:AssertionError: optimizer.zero_grad() was called after loss.backward() but before optimizer.step() or optimizer.synchronize(). This is prohibited as it can cause a race condition.
[1,7]<stderr>:Traceback (most recent call last):
[1,7]<stderr>:  File "main.py", line 39, in <module>
[1,7]<stderr>:    train(args)
[1,7]<stderr>:  File "/scratch/hpc90a02/SRGANT/tk4h/mode.py", line 175, in train
[1,7]<stderr>:    g_optim.zero_grad()
[1,7]<stderr>:  File "/scratch/hpc90a02/.conda/envs/gs/lib/python3.6/site-packages/horovod/torch/optimizer.py", line 339, in zero_grad
[1,7]<stderr>:    raise AssertionError("optimizer.zero_grad() was called after loss.backward() "
[1,7]<stderr>:AssertionError: optimizer.zero_grad() was called after loss.backward() but before optimizer.step() or optimizer.synchronize(). This is prohibited as it can cause a race condition.
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[61481,1],2]
  Exit code:    1
--------------------------------------------------------------------------
